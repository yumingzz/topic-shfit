import os
import random
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import tqdm

# å®šä¹‰æ•°æ®é›†ç›®å½•
# datasets = {
#     "colab": "dataset/raw_data/colab/",
#     "enron": "dataset/raw_data/enron/",
#     "facebook": "dataset/raw_data/facebook/"
# }
#
# # å®šä¹‰è¾“å‡ºç›®å½•
# output_dir_base = "sir_results/"
# os.makedirs(output_dir_base, exist_ok=True)


# è·å–æ—¶é—´æ­¥ï¼ˆè·å–ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶ï¼‰
def get_time_steps(raw_data_dir):
    return sorted([f for f in os.listdir(raw_data_dir) if f.endswith(".txt")])


# åŠ è½½æ— å‘å›¾
def load_graph_from_file(file_path):
    G = nx.Graph()
    with open(file_path, "r") as f:
        for line in f:
            parts = line.split()
            if len(parts) >= 2:
                u, v = map(int, parts[:2])
                G.add_edge(u, v)
    return G


# SIR ä¼ æ’­æ¨¡å‹
def SIR(G, infected, beta, miu=1, iterations=1000):
    """ è®¡ç®—å•ä¸ªèŠ‚ç‚¹çš„ SIR å½±å“åŠ› """
    re = 0
    for _ in range(iterations):
        inf = set(infected)  # åˆå§‹æ„ŸæŸ“é›†åˆ
        R = set()  # æ¢å¤é›†åˆ

        while inf:
            newInf = []
            for i in inf:
                for j in G.neighbors(i):
                    if random.uniform(0, 1) < beta and j not in inf and j not in R:
                        newInf.append(j)

                # åˆ¤æ–­æ˜¯å¦æ¢å¤
                if random.uniform(0, 1) > miu:
                    newInf.append(i)
                else:
                    R.add(i)

            inf = set(newInf)

        re += len(R) + len(inf)

    return re / iterations


# è®¡ç®—æ•´ä¸ªç½‘ç»œæ‰€æœ‰èŠ‚ç‚¹çš„ SIR å½±å“åŠ›
def SIR_dict(G, beta=0.1, miu=1, real_beta=None, a=1.5):
    node_list = list(G.nodes())
    SIR_dic = {}

    if real_beta:
        dc_list = np.array(list(dict(G.degree()).values()))
        beta = a * (dc_list.mean() / (dc_list.var() + dc_list.mean()))

    print(f"âœ… è®¡ç®—ä¼ æ’­æ¦‚ç‡ beta: {beta}")

    for node in tqdm(node_list):
        SIR_dic[node] = SIR(G, infected=[node], beta=beta, miu=miu)

    return SIR_dic


# å­˜å‚¨ SIR è®¡ç®—ç»“æœ
def save_sir_dict(dic, path):
    df = pd.DataFrame({'Node': list(dic.keys()), 'SIR': list(dic.values())})
    df.to_csv(path, index=False)


# å¤„ç†æ‰€æœ‰æ—¶é—´æ­¥çš„ SIR è®¡ç®—
def process_all_time_steps(datasets, output_dir_base, a_list):
    for dataset_name, raw_data_dir in datasets.items():
        print(f"ğŸš€ å¤„ç†æ•°æ®é›†: {dataset_name}")
        output_dir = os.path.join(output_dir_base, dataset_name)
        os.makedirs(output_dir, exist_ok=True)

        time_steps = get_time_steps(raw_data_dir)
        print(f"ğŸ“‚ å‘ç° {len(time_steps)} ä¸ªæ–‡ä»¶: {time_steps}")

        for file_name in time_steps:
            file_path = os.path.join(raw_data_dir, file_name)
            G = load_graph_from_file(file_path)

            for idx, a in enumerate(a_list):
                print(f"ğŸ“Œ å¤„ç† {file_name}, ä¼ æ’­å‚æ•° a={a} ...")
                sir_dict = SIR_dict(G, real_beta=True, a=a)
                output_file = os.path.join(output_dir, f"{file_name.replace('.txt', '')}_a[{a}].csv")

                save_sir_dict(sir_dict, output_file)
                print(f"âœ… ç»“æœå·²ä¿å­˜è‡³ {output_file}")



# è¿è¡Œè®¡ç®—
# process_all_time_steps(datasets, output_dir_base)
# print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†çš„ SIR è®¡ç®—å®Œæˆï¼")