import os
import random
import time
import argparse
import configparser
from typing import List, Dict, Union
import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from data import get_data
from model import NodeImportanceModel  # ä½¿ç”¨æ–°çš„æ¨¡å‹ç±»
from eval import evaluate
from itertools import chain, repeat
from pre_data import split_txt_by_timestamp,rename_and_move_txt_files,generate_ba_snapshots,generate_dynamic_ba_snapshots
from SIR import process_all_time_steps
# Setting random seeds for reproducibility
random.seed(23)
np.random.seed(23)
torch.manual_seed(23)
import csv
import networkx as nx
from data import compute_D_features
from data import compute_H_features
import community as community_louvain
from data import compute_community_D_features
from data import compute_community_H_features

def train(
        model: torch.nn.Module,
        train_dataset: List[Data],
        optimizer: torch.optim.Optimizer,
        scheduler: torch.optim.lr_scheduler.LRScheduler,
        hparams: Dict[str, Union[int, float]],
        model_path: str
) -> torch.nn.Module:
    """
    ä½¿ç”¨æä¾›çš„è®­ç»ƒæ•°æ®é›†è®­ç»ƒç»™å®šçš„æ¨¡å‹ã€‚

    å‚æ•°:
        model (torch.nn.Module): è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
        train_dataset (List[Data]): ç”¨äºè®­ç»ƒçš„ PyTorch Geometric Data å¯¹è±¡åˆ—è¡¨ã€‚
        optimizer (torch.optim.Optimizer): è®­ç»ƒä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€‚
        scheduler (torch.optim.lr_scheduler.LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
        hparams (Dict[str, Union[int, float]]): è®­ç»ƒçš„è¶…å‚æ•°ï¼ŒåŒ…æ‹¬è®­ç»ƒå‘¨æœŸæ•°ã€alphaã€beta ç­‰ã€‚
        model_path (str): ä¿å­˜è®­ç»ƒåæ¨¡å‹çš„è·¯å¾„ã€‚

    è¿”å›:
        torch.nn.Module: è®­ç»ƒåçš„æ¨¡å‹ã€‚
    """
    print(f"=========== Train ===========")
    best_train_loss = float('inf')
    model.train()
    for epoch in range(hparams["epochs"]):
        start_time = time.time()
        total_loss = 0
        for data in train_dataset:
            optimizer.zero_grad()
            loss = model.compute_total_loss(data)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataset)
        print("[*] epoch: {}, loss: {:.4f}, time: {:.1f}".format(epoch, avg_loss, time.time() - start_time))

        scheduler.step(avg_loss)

        # å¦‚æœå½“å‰å‘¨æœŸçš„æŸå¤±å€¼ä½äºæœ€ä½³æŸå¤±å€¼ï¼Œåˆ™æ›´æ–°æœ€ä½³æŸå¤±å€¼å¹¶ä¿å­˜æ¨¡å‹
        if avg_loss < best_train_loss:
            best_train_loss = avg_loss
            print('[*] --> Best training loss {:.4f} reached at epoch {}.'.format(avg_loss, epoch))
            print(f"[*] --> Saving the model {model_path}")
            torch.save(model.state_dict(), model_path)

    print("[*] Training is completed.")
    return model


def load_hparams() -> Dict[str, Union[int, float]]:
    """
    ä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½è¶…å‚æ•°ã€‚

    ä»ä¸è„šæœ¬ä½äºåŒä¸€ç›®å½•ä¸‹çš„ "config.ini" æ–‡ä»¶ä¸­è¯»å–è¶…å‚æ•°ã€‚
    è¶…å‚æ•°åº”ä½äºæ–‡ä»¶çš„ "hyperparameters" éƒ¨åˆ†ã€‚

    è¿”å›:
        Dict[str, Union[int, float]]: åŒ…å«è¶…å‚æ•°çš„å­—å…¸ï¼Œé”®åŒ…æ‹¬ï¼š
            - "epochs": è®­ç»ƒçš„å‘¨æœŸæ•°ã€‚
            - "train_test_ratio": è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„æ¯”ä¾‹ã€‚
            - "hidden_dim": éšè—å±‚çš„ç»´åº¦ã€‚
            - "output_dim": è¾“å‡ºå±‚çš„ç»´åº¦ã€‚
            - "alpha": å›¾é‡å»ºæŸå¤±çš„æƒé‡ã€‚
            - "beta": å¯¹æ¯”é¢„æµ‹ç¼–ç æŸå¤±çš„æƒé‡ã€‚
            - "learning_rate": ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€‚
            - "weight_decay": ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰ã€‚
            - "scheduler_patience": å­¦ä¹ ç‡è°ƒåº¦å™¨åœ¨å¤šå°‘ä¸ªå‘¨æœŸå†…æ²¡æœ‰æ”¹è¿›åä¼šé™ä½å­¦ä¹ ç‡ã€‚
            - "scheduler_factor": å­¦ä¹ ç‡è°ƒåº¦å™¨é™ä½å­¦ä¹ ç‡çš„å› å­ã€‚
            - "scheduler_min_lr": å­¦ä¹ ç‡è°ƒåº¦å™¨å…è®¸çš„æœ€å°å­¦ä¹ ç‡ã€‚

    å¼‚å¸¸:
        configparser.NoSectionError: å¦‚æœé…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ "hyperparameters" éƒ¨åˆ†ã€‚
        configparser.NoOptionError: å¦‚æœ "hyperparameters" éƒ¨åˆ†ä¸­ç¼ºå°‘ä»»ä½•é¢„æœŸçš„é€‰é¡¹ã€‚
    """
    hparams = configparser.ConfigParser()
    hparams.read("config.ini")

    hparams = {
        "epochs": hparams.getint("hyperparameters", "EPOCHS"),
        "train_test_ratio": hparams.getfloat("hyperparameters", "TRAIN_TEST_RATIO"),
        "hidden_dim": hparams.getint("hyperparameters", "HIDDEN_DIM"),
        "output_dim": hparams.getint("hyperparameters", "OUTPUT_DIM"),
        "learning_rate": hparams.getfloat("hyperparameters", "LEARNING_RATE"),
        "weight_decay": hparams.getfloat("hyperparameters", "WEIGHT_DECAY"),
        "scheduler_patience": hparams.getint("hyperparameters", "SCHEDULER_PATIENCE"),
        "scheduler_factor": hparams.getfloat("hyperparameters", "SCHEDULER_FACTOR"),
        "scheduler_min_lr": hparams.getfloat("hyperparameters", "SCHEDULER_MIN_LR"),
    }
    return hparams


def inference(
    model: torch.nn.Module,
    dataset: List[Data],
    test_timesteps: List[int],
    model_path: str,
    device: torch.device
) -> List[torch.Tensor]:
    """
    ä½¿ç”¨æ¨¡å‹å¯¹æ¯ä¸ªæ—¶é—´æ­¥æ»‘åŠ¨çª—å£é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„èŠ‚ç‚¹é‡è¦æ€§ï¼ˆSIRï¼‰ã€‚

    è¿”å›:
        List[torch.Tensor]: æ¯ä¸ªæµ‹è¯•æ—¶é—´æ­¥çš„é¢„æµ‹å¼ é‡åˆ—è¡¨ã€‚
    """
    print(f"[*] Loading model from: {model_path}")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    test_preds = []
    for t in test_timesteps:
        if t == 0:
            continue  # t=0 æ²¡æœ‰å†å²ï¼Œè·³è¿‡

        print(f"[*] Predicting timestep {t} using history [0:{t}]...")
        with torch.no_grad():
            y_pred = model.predict(graph=dataset[t], normalize=True)
        test_preds.append(y_pred.cpu())
    return test_preds


def main():
    """
    ä¸»å‡½æ•°ï¼Œè¿è¡Œè®­ç»ƒå’Œæ¨ç†æµç¨‹ã€‚

    1. é…ç½®è·¯å¾„å’Œè¶…å‚æ•°ã€‚
    2. åŠ è½½æ•°æ®é›†ã€‚
    3. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    4. è®­ç»ƒæ¨¡å‹ã€‚
    5. å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†ã€‚
    6. è¯„ä¼°æµ‹è¯•ç»“æœã€‚
    """
    # é…ç½®æ•°æ®é›†å’Œæ¨¡å‹è·¯å¾„
    parser = argparse.ArgumentParser(description='Process dataset name.')
    parser.add_argument(
        '--dataset_name',
        default='tiage',
        choices=['enron', 'facebook', 'colab','BA','DBLP5','reddit','birth','expansion','merge','DBLP3','tiage'],
        help='Specify the dataset name (options: enron, facebook, colab). Default is "enron".'
    )
    args = parser.parse_args()

    # è¯»å–æ•°æ®é›†åç§°å‚æ•°
    dataset_name = args.dataset_name
    print(f'[*] Dataset name selected: {dataset_name}')
    model_dir = os.path.join("model_registry")
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, f"node_importance_{dataset_name}.pkl")

    # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨ GPUï¼Œå¦åˆ™ä½¿ç”¨ CPUï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f'[*] Device selected: {device}')

    # åŠ è½½è¶…å‚æ•°
    hparams = load_hparams()
    # å¾ªç¯æµ‹è¯•ä¸åŒaä¸‹é¢çš„æ•ˆæœ
    # a_list = [1.5, 1.1, 1.2, 1.3, 1.4, 1.0, 1.6, 1.7, 1.8, 1.9]
    a_list = [1.5, 1.1, 1.2, 1.3, 1.4, 1.0, 1.6, 1.7, 1.8,1.9]
    df = pd.DataFrame(columns=["Timestep", "KendallTau", "MAP@10", "MAP@20"])
    df = pd.DataFrame(columns=["Timestep", "KendallTau", "MAP"])
    for i in range(0, len(a_list)):
        # åŠ è½½æ•°æ®é›†
        dataset, train_timesteps, test_timesteps = get_data(dataset_name=dataset_name,
                                                            train_test_ratio=hparams["train_test_ratio"],
                                                            device=device, a=a_list[i])
        # print(test_timesteps)
        train_dataset = [dataset[k] for k in train_timesteps]

        INPUT_DIM = dataset[0].x.size(1)

        # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        model = NodeImportanceModel(input_dim=INPUT_DIM,
                                    hidden_dim=hparams["hidden_dim"],
                                    output_dim=hparams["output_dim"]).to(device=device)
        optimizer = torch.optim.Adam(model.parameters(),
                                     lr=hparams["learning_rate"],
                                     weight_decay=hparams["weight_decay"])
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer=optimizer,
            patience=hparams["scheduler_patience"],
            factor=hparams["scheduler_factor"],
            min_lr=hparams["scheduler_min_lr"]
        )
        if i == 0:
            # è®­ç»ƒæ¨¡å‹
            model = train(model=model,
                          train_dataset=train_dataset,
                          optimizer=optimizer,
                          scheduler=scheduler,
                          hparams=hparams,
                          model_path=model_path)

        # å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†
        test_probs = inference(model=model,
                               dataset=dataset,
                               test_timesteps=test_timesteps,
                               model_path=model_path,
                               device=device)

        # è¯„ä¼°æµ‹è¯•ç»“æœ
        test_results = evaluate(test_probs=test_probs,
                                test_timesteps=test_timesteps,
                                dataset=dataset)
        print(test_results)
        df = pd.concat([df, test_results], ignore_index=True)
        print(df)
    # ä½¿ç”¨ itertools.chain å’Œ itertools.repeat é‡å¤æ¯ä¸ªå…ƒç´ ä¸‰æ¬¡
    t_values = list(chain.from_iterable(repeat(item, len(test_timesteps)) for item in a_list))
    # æ·»åŠ æ–°åˆ— a
    df['a'] = t_values
    # æŒ‰ç…§åˆ— 'a' é™åºæ’åˆ—
    df_sorted = df.sort_values(by='a', ascending=True)
    # csv_file = 'tau/'  + dataset_name + '.csv'
    csv_file = 'å­¦ä¹ ç‡0.05/' + dataset_name + '.csv'
    # csv_file = 'å‚æ•°0.25/' + dataset_name + '.csv'
    df_sorted.to_csv(csv_file, index=False)
    #
    # learning_rates = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4]
    # df_all = pd.DataFrame(columns=["LearningRate", "Timestep", "KendallTau", "MAP", "a"])
    # for lr in learning_rates:
    #     print(f"\n=== æ­£åœ¨æµ‹è¯•å­¦ä¹ ç‡: {lr} ===\n")
    #     for i in range(0, len(a_list)):
    #         # åŠ è½½æ•°æ®é›†
    #         dataset, train_timesteps, test_timesteps = get_data(
    #             dataset_name=dataset_name,
    #             train_test_ratio=hparams["train_test_ratio"],
    #             device=device,
    #             a=a_list[i]
    #         )
    #         train_dataset = [dataset[k] for k in train_timesteps]
    #         INPUT_DIM = dataset[0].x.size(1)
    #
    #         # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
    #         model = NodeImportanceModel(
    #             input_dim=INPUT_DIM,
    #             hidden_dim=hparams["hidden_dim"],
    #             output_dim=hparams["output_dim"]
    #         ).to(device=device)
    #
    #         optimizer = torch.optim.Adam(
    #             model.parameters(),
    #             lr=lr,  # ä½¿ç”¨å½“å‰å¾ªç¯çš„å­¦ä¹ ç‡
    #             weight_decay=hparams["weight_decay"]
    #         )
    #
    #         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    #             optimizer=optimizer,
    #             patience=hparams["scheduler_patience"],
    #             factor=hparams["scheduler_factor"],
    #             min_lr=hparams["scheduler_min_lr"]
    #         )
    #
    #         if i == 0:
    #             # è®­ç»ƒæ¨¡å‹
    #             model = train(
    #                 model=model,
    #                 train_dataset=train_dataset,
    #                 optimizer=optimizer,
    #                 scheduler=scheduler,
    #                 hparams=hparams,
    #                 model_path=model_path
    #             )
    #
    #         # æ¨ç†
    #         test_probs = inference(
    #             model=model,
    #             dataset=dataset,
    #             test_timesteps=test_timesteps,
    #             model_path=model_path,
    #             device=device
    #         )
    #
    #         # è¯„ä¼°
    #         test_results = evaluate(
    #             test_probs=test_probs,
    #             test_timesteps=test_timesteps,
    #             dataset=dataset
    #         )
    #
    #         test_results['LearningRate'] = lr
    #         test_results['a'] = a_list[i]
    #         df_all = pd.concat([df_all, test_results], ignore_index=True)
    # save_path = f'å­¦ä¹ ç‡å®éªŒç»“æœ/{dataset_name}_lr_experiment.csv'
    # os.makedirs(os.path.dirname(save_path), exist_ok=True)
    # df_all.to_csv(save_path, index=False)
    # print(f"âœ… å­¦ä¹ ç‡å®éªŒç»“æœå·²ä¿å­˜è‡³ {save_path}")


def split_csv_to_txt_files(csv_file_path: str, output_dir: str, num_files: int = 10) -> None:
    """
    è¯»å– CSV æ–‡ä»¶ï¼Œåˆ é™¤ç¬¬ä¸€è¡Œï¼Œå¹¶å°†å‰©ä½™å†…å®¹å¹³å‡åˆ’åˆ†ä¸ºå¤šä¸ª .txt æ–‡ä»¶ã€‚

    Args:
        csv_file_path (str): è¾“å…¥çš„ CSV æ–‡ä»¶è·¯å¾„ã€‚
        output_dir (str): è¾“å‡ºçš„ .txt æ–‡ä»¶ä¿å­˜ç›®å½•ã€‚
        num_files (int): è¦åˆ’åˆ†çš„æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤ä¸º 10ã€‚
    """
    # è¯»å– CSV æ–‡ä»¶ï¼Œè·³è¿‡ç¬¬ä¸€è¡Œ
    # df = pd.read_csv(csv_file_path, skiprows=1)
    #
    # # # è·å–æ€»è¡Œæ•°
    # # total_rows = len(df)
    # # # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°
    # # rows_per_file = total_rows // num_files
    #
    # # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    # os.makedirs(output_dir, exist_ok=True)
    # output_file_path = os.path.join(output_dir, f"Gnu_1.txt")
    # df.to_csv(output_file_path, index=False, header=False, sep="\t")
    #
    # print(f"Saved {output_file_path} with {len(df)} rows.")

    # åˆ†å‰²æ•°æ®å¹¶ä¿å­˜ä¸º .txt æ–‡ä»¶
    # for i in range(num_files):
    #     start_row = i * rows_per_file
    #     end_row = (i + 1) * rows_per_file if i < num_files - 1 else total_rows
    #
    #     # è·å–å½“å‰æ–‡ä»¶çš„æ•°æ®
    #     current_df = df.iloc[start_row:end_row]
    #
    #     # ä¿å­˜ä¸º .txt æ–‡ä»¶
    #     output_file_path = os.path.join(output_dir, f"Gnutella_{i}.txt")
    #     current_df.to_csv(output_file_path, index=False, header=False, sep="\t")
    #
    #     print(f"Saved {output_file_path} with {len(current_df)} rows.")

def rename_files(directory, prefix="expansion"):
    """
    éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šçš„å‘½åè§„åˆ™é‡å‘½åæ–‡ä»¶ã€‚

    å‚æ•°:
    directory (str): è¦éå†çš„ç›®å½•è·¯å¾„
    prefix (str): æ–‡ä»¶åå‰ç¼€ï¼Œé»˜è®¤ä¸º "birth_death_data"
    """
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(directory):
        print(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]

    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆå¯é€‰ï¼‰
    files.sort()

    # é‡å‘½åæ–‡ä»¶
    for index, filename in enumerate(files):
        # æ„é€ æ–°çš„æ–‡ä»¶å
        new_filename = f"{prefix}_{index}.txt"
        new_file_path = os.path.join(directory, new_filename)

        # è·å–åŸå§‹æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        old_file_path = os.path.join(directory, filename)

        # é‡å‘½åæ–‡ä»¶
        os.rename(old_file_path, new_file_path)
        print(f"æ–‡ä»¶ {filename} å·²é‡å‘½åä¸º {new_filename}")
def rename_files_in_directory(directory, old_prefix="merge_split_data", new_prefix="merge"):
    """
    éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ .csv æ–‡ä»¶ï¼Œå¹¶å°†æ–‡ä»¶åä¸­çš„ old_prefix æ›¿æ¢ä¸º new_prefixã€‚

    å‚æ•°:
    directory (str): è¦éå†çš„ç›®å½•è·¯å¾„
    old_prefix (str): åŸå§‹æ–‡ä»¶åå‰ç¼€ï¼Œé»˜è®¤ä¸º "birth_death_data"
    new_prefix (str): æ–°æ–‡ä»¶åå‰ç¼€ï¼Œé»˜è®¤ä¸º "birth"
    """
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(directory):
        print(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return

    # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    files = [f for f in os.listdir(directory) if f.endswith('.csv')]

    # é‡å‘½åæ–‡ä»¶
    for filename in files:
        if filename.startswith(old_prefix):
            # æ„é€ æ–°çš„æ–‡ä»¶å
            new_filename = filename.replace(old_prefix, new_prefix)
            old_file_path = os.path.join(directory, filename)
            new_file_path = os.path.join(directory, new_filename)

            # é‡å‘½åæ–‡ä»¶
            os.rename(old_file_path, new_file_path)
            print(f"æ–‡ä»¶ {filename} å·²é‡å‘½åä¸º {new_filename}")
if __name__ == "__main__":
    # ç¤ºä¾‹è°ƒç”¨ æ•°æ®é¢„å¤„ç†
    # csv_file_path = "datasets/raw_data/BA-8k.csv"  # è¾“å…¥çš„ CSV æ–‡ä»¶è·¯å¾„
    # output_dir = "datasets/raw_data/Gnu"  # è¾“å‡ºçš„ .txt æ–‡ä»¶ä¿å­˜ç›®å½•
    # split_csv_to_txt_files(csv_file_path, output_dir)
    # # ç¤ºä¾‹è°ƒç”¨
    # split_txt_by_timestamp("datasets/raw_data/Dynamic_PPIN.txt", "datasets/raw_data/Dynamic_PPIN/")
    # ç¤ºä¾‹è°ƒç”¨
    # source_directory = r"D:\lesson\Postgraduate\EXcode\DGCN3\MRCNN"
    # target_directory = r"D:\lesson\Postgraduate\EXcode\DGCN3\datasets\raw_data\Gnu"
    # rename_and_move_txt_files(source_directory, target_directory)

    # # å®šä¹‰è¦éå†çš„ç›®å½•
    # directory = 'datasets/raw_data/expansion'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„\
    # # è°ƒç”¨å‡½æ•°
    # rename_files(directory)

    # å®šä¹‰è¦éå†çš„ç›®å½•
    # directory = 'sir_results/merge'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„
    # # è°ƒç”¨å‡½æ•°
    # rename_files_in_directory(directory)

    # generate_ba_snapshots(6400, 8000, 3, "datasets/raw_data/BA")
    # generate_dynamic_ba_snapshots(
    #     num_snapshots=10,
    #     max_nodes=1000,
    #     m=3,
    #     output_dir="datasets/raw_data/BA"
    # )

    # æ„é€ SIR
    # å®šä¹‰æ•°æ®é›†ç›®å½•
    # datasets = {
    #     # "merge": "datasets/raw_data/merge",
    #     # "expansion":"datasets/raw_data/expansion",
    #     # "birth":"datasets/raw_data/birth"
    #     # "BA": "datasets/raw_data/BA"
    #     "tiage": "datasets/raw_data/tiage"
    # }
    #
    # # å®šä¹‰è¾“å‡ºç›®å½•
    # output_dir_base = "sir_results/"
    # os.makedirs(output_dir_base, exist_ok=True)
    # a_list = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]
    # # è¿è¡Œè®¡ç®—
    # process_all_time_steps(datasets, output_dir_base, a_list= a_list)
    # print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†çš„ SIR è®¡ç®—å®Œæˆï¼")
    #
    # print(torch.__version__)
    # print(torch.cuda.is_available())
    # print(f"PyTorch Version: {torch.__version__}")  # æ‰“å° PyTorch ç‰ˆæœ¬å·
    # print(f"CUDA Available: {torch.cuda.is_available()}")  # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU è®¾å¤‡
    # if torch.cuda.is_available():
    #     print(f"CUDA Version: {torch.version.cuda}")  # è¾“å‡ºç¼–è¯‘æ—¶ä½¿ç”¨çš„ CUDA ç‰ˆæœ¬
    #     print(f"Device Name: {torch.cuda.get_device_name(0)}")  # è·å–è®¾å¤‡åç§°
    # else:
    #     print("No CUDA-enabled device found.")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    main()
    end_time = time.time()
    # è®¡ç®—è¿è¡Œæ—¶é—´(ç§’)
    elapsed_time = end_time - start_time
    # å°†è¿è¡Œæ—¶é—´è½¬æ¢ä¸ºå°æ—¶
    runtime_minutes = elapsed_time / 60
    # æ‰“å°è¿è¡Œæ—¶é—´ï¼ˆç§’å’Œå°æ—¶ï¼‰
    print(f"ä»£ç è¿è¡Œæ—¶é—´ï¼š{elapsed_time:.6f} ç§’")
    print(f"ä»£ç è¿è¡Œæ—¶é—´ï¼š{runtime_minutes:.6f} åˆ†é’Ÿ")

    # # å®šä¹‰è¾¹åˆ—è¡¨
    # edges = [
    #     (0, 1),
    #     (0, 2),
    #     (0, 3),
    #     (1, 3),
    #     (2, 3),
    #     (2, 5),
    #     (4, 5),
    #     (4, 6),
    #     (5, 6)
    # ]
    # # åˆ›å»ºå›¾å¯¹è±¡
    # G = nx.Graph()
    # G.add_edges_from(edges)
    # num_nodes =  7
    # # å¯è§†åŒ–æˆ–æ‰“å°ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    # print("èŠ‚ç‚¹æ•°:", G.number_of_nodes())
    # # print("è¾¹æ•°:", G.number_of_edges())
    # # print("èŠ‚ç‚¹åˆ—è¡¨:", list(G.nodes))
    # # print("è¾¹åˆ—è¡¨:", list(G.edges))
    #
    # # è®¡ç®—èŠ‚ç‚¹çš„ç»“æ„ç‰¹å¾
    # deg_features = compute_D_features(G, num_nodes)
    # h_features = compute_H_features(G, num_nodes)
    # partition = community_louvain.best_partition(G)
    # # å°† Louvain ç®—æ³•çš„ç»“æœè½¬æ¢ä¸ºä¸ greedy_modularity_communities ç›¸åŒçš„æ ¼å¼
    # # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºç¤¾å›¢ç¼–å·ï¼Œå€¼ä¸ºè¯¥ç¤¾å›¢çš„èŠ‚ç‚¹é›†åˆ
    # community_dict = {}
    # for node, community_id in partition.items():
    #     if community_id not in community_dict:
    #         community_dict[community_id] = set()
    #     community_dict[community_id].add(node)
    # # å°†ç¤¾å›¢å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé›†åˆ
    # communities = list(community_dict.values())
    # # _,  _, communities, _, _ = bayanpy.bayan(G, threshold=0.001,time_allowed=60, resolution=1)
    #
    # # communities = list(greedy_modularity_communities(G))
    #
    # # è®¡ç®—ç¤¾å›¢å†…çš„ç»“æ„ç‰¹å¾
    # comm_deg_features = compute_community_D_features(G, communities, num_nodes)
    # comm_h_features = compute_community_H_features(G, communities, num_nodes)
    #
    # # åŸå§‹å•ä½ç‰¹å¾
    # # onehot_tensor = torch.eye(num_nodes)
    #
    # # æœ€ç»ˆæ‹¼æ¥ç‰¹å¾
    # # final_x = torch.cat([onehot_tensor, deg_features, comm_deg_features,h_features,comm_h_features], dim=1)
    # final_x = torch.cat([deg_features, comm_deg_features, h_features, comm_h_features], dim=1)
    # print(final_x)



    # import os
    # import shutil
    # #
    # # # å®šä¹‰æ˜ å°„å…³ç³»
    # mapping = {
    #     "GrQ": "Gnu_3",
    #     "Hep": "Gnu_4",
    #     "Email": "Gnu_5",
    #     "Faa": "Gnu_6",
    #     "Facebook": "Gnu_7",
    #     "Figeys": "Gnu_8",
    #     "jazz": "Gnu_9",
    #     "LastFM": "Gnu_10",
    #     "NS": "Gnu_11",
    #     "powergrid": "Gnu_12",
    #     "Sex": "Gnu_13",
    #     "stelzl": "Gnu_14",
    #     "vidal": "Gnu_15"
    # }
    #
    # # å®šä¹‰ä¼ æ’­æ¦‚ç‡å€æ•°åˆ—è¡¨
    # a_list = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9]
    #
    # # æºç›®å½•å’Œç›®æ ‡ç›®å½•
    # source_dir = "MRCNN/SIR results"
    # target_dir = "sir_results/Gnu"
    #
    # # åˆ›å»ºç›®æ ‡ç›®å½•
    # if not os.path.exists(target_dir):
    #     os.makedirs(target_dir)
    #
    # # éå†æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å¤¹
    # for folder_name in os.listdir(source_dir):
    #     folder_path = os.path.join(source_dir, folder_name)
    #
    #     # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•
    #     if os.path.isdir(folder_path):
    #         # è·å–æ˜ å°„åçš„åç§°
    #         if folder_name in mapping:
    #             mapped_name = mapping[folder_name]
    #         else:
    #             print(f"Warning: {folder_name} not found in mapping. Skipping.")
    #             continue
    #
    #         # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    #         for file_name in os.listdir(folder_path):
    #             file_path = os.path.join(folder_path, file_name)
    #
    #             # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶
    #             if os.path.isfile(file_path):
    #                 # è·å–æ–‡ä»¶ç¼–å·ï¼ˆä»æ–‡ä»¶åä¸­æå–ï¼‰
    #                 file_index = int(file_name.split('_')[1].split('.')[0])
    #
    #                 # è·å–å¯¹åº”çš„ä¼ æ’­æ¦‚ç‡å€æ•°
    #                 a_value = a_list[file_index]
    #
    #                 # æ„é€ æ–°çš„æ–‡ä»¶å
    #                 new_file_name = f"{mapped_name}_a[{a_value}].csv"
    #                 new_file_path = os.path.join(target_dir, new_file_name)
    #
    #                 # å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•å¹¶é‡å‘½å
    #                 shutil.copyfile(file_path, new_file_path)
    #                 print(f"Renamed {file_name} to {new_file_name}")
    #
    # print("Renaming complete.")

